Getting Started

Amazon Bedrock is a fully managed service that enables developers to build and deploy generative AI applications using pretrained foundation models (FMs).

Prompt Flows are visual tools or frameworks within Amazon Bedrock that allow us to orchestrate and optimize prompt engineering workflows for interacting with foundation models without using any code. Prompt Flows provide the following:

Design complex workflows: Create step-by-step processes to tailor foundation models’ responses to specific queries.

Experiment with prompts: Test and refine different prompts to achieve optimal responses for our application.

Integrate multiple components: Combine models, functions, and APIs in a unified flow to enhance the functionality of generative AI applications.




Types of integrations provided by Prompt Flows
We can integrate the following components into Bedrock Prompt Flows:

Foundation models (FMs): Prompts serve as the input instructions or queries provided to FMs to generate responses tailored to specific tasks such as text summarization, classification, or creative writing. They enable fine-tuned interaction with pretrained models, ensuring the AI system understands the task and produces relevant outputs.

Agents: Agents act as orchestrators, enabling multistep workflows by combining FMs’ capabilities with other components such as APIs, databases, and decision-making logic. They handle dynamic and complex interactions, such as gathering information from multiple sources or executing conditional operations based on user input.

AWS Lambda functions: Lambda functions execute custom business logic, data processing, or backend operations within the prompt flow, extending FMs’ functionality. They enable on-demand execution of serverless operations, such as processing intermediate data or interacting with external APIs.

Knowledge Bases: A knowledge base provides structured data or domain-specific information that FMs can use to improve response accuracy and context. They offer a centralized repository of relevant information, enabling consistent and accurate answers across workflows.

Amazon Lex: Amazon Lex adds conversational capabilities to workflows, enabling natural language understanding for chatbots and virtual assistants. Lex provides robust conversational AI for multi-turn dialogue, integrating seamlessly with FMs and other AWS services.

Operations: Operations like iterators and collectors manage data flow and execution logic within prompt flows, handling tasks like looping through datasets or aggregating results.

Why we use them: They provide a structured approach to handling complex workflows, ensuring data is processed systematically.

Efficiency of operations: These simplify multistep processes, making workflows more modular, reusable, and easier to debug.

S3: S3 can act as a data source. We can store and retrieve intermediate data during flow execution.

By integrating these components, Amazon Bedrock Prompt Flows provides a highly modular and scalable framework to create sophisticated generative AI workflows, optimized for various applications.

General instructions
Here are some general guidelines that will help you complete this lab:

Before starting this lab, ensure that you’re working within the us-east-1 region because access to resources outside of this region has been restricted for this lab. On the AWS Management Console, click the region drop-down menu at the top-right corner next to your username and select the “US East (N. Virginia)” option.

Using the same resource names as suggested in the lab is essential. For instance, if you are instructed to name a function as my_function, choosing any other name for the function may not be possible.

There are limited permissions to attempt this lab, and some console pages may display insufficient privilege error messages. These can be safely ignored.

Now that the instructions have been understood let’s get started.